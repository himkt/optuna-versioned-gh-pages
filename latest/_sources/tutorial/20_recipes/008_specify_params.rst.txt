
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorial/20_recipes/008_specify_params.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorial_20_recipes_008_specify_params.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorial_20_recipes_008_specify_params.py:


.. _specify_params:

Specify Hyperparameters Manually
================================

It's natural that you have some specific sets of hyperparameters to try first such as initial learning rate
values and the number of leaves.
Also, it's also possible that you've already tried those sets before having Optuna find better
sets of hyperparameters.

Optuna provides two APIs to support such cases:

1. Passing those sets of hyperparameters and let Optuna evaluate them - :func:`~optuna.study.Study.enqueue_trial`
2. Adding the results of those sets as completed ``Trial``\s - :func:`~optuna.study.Study.add_trial`

First Scenario: Have Optuna evaluate your hyperparameters
---------------------------------------------------------

In this scenario, let's assume you have some out-of-box sets of hyperparameters but have not
evaluated them yet and decided to use Optuna to find better sets of hyperparameters.

Optuna has :func:`optuna.study.Study.enqueue_trial` which lets you pass those sets of
hyperparameters to Optuna and Optuna will evaluate them.

This section walks you through how to use this lit API with `LightGBM <https://lightgbm.readthedocs.io/en/latest/>`_.

.. GENERATED FROM PYTHON SOURCE LINES 28-38

.. code-block:: default


    import lightgbm as lgb
    import numpy as np
    import sklearn.datasets
    import sklearn.metrics
    from sklearn.model_selection import train_test_split

    import optuna









.. GENERATED FROM PYTHON SOURCE LINES 39-40

Define the objective function.

.. GENERATED FROM PYTHON SOURCE LINES 40-68

.. code-block:: default

    def objective(trial):
        data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)
        train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)
        dtrain = lgb.Dataset(train_x, label=train_y)
        dvalid = lgb.Dataset(valid_x, label=valid_y)

        param = {
            "objective": "binary",
            "metric": "auc",
            "verbosity": -1,
            "boosting_type": "gbdt",
            "bagging_fraction": min(trial.suggest_float("bagging_fraction", 0.4, 1.0 + 1e-12), 1),
            "bagging_freq": trial.suggest_int("bagging_freq", 0, 7),
            "min_child_samples": trial.suggest_int("min_child_samples", 5, 100),
        }

        # Add a callback for pruning.
        pruning_callback = optuna.integration.LightGBMPruningCallback(trial, "auc")
        gbm = lgb.train(
            param, dtrain, valid_sets=[dvalid], verbose_eval=False, callbacks=[pruning_callback]
        )

        preds = gbm.predict(valid_x)
        pred_labels = np.rint(preds)
        accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)
        return accuracy









.. GENERATED FROM PYTHON SOURCE LINES 69-70

Then, construct ``Study`` for hyperparameter optimization.

.. GENERATED FROM PYTHON SOURCE LINES 70-73

.. code-block:: default


    study = optuna.create_study(direction="maximize", pruner=optuna.pruners.MedianPruner())








.. GENERATED FROM PYTHON SOURCE LINES 74-76

Here, we get Optuna evaluate some sets with larger ``"bagging_fraq"`` value and
the default values.

.. GENERATED FROM PYTHON SOURCE LINES 76-100

.. code-block:: default


    study.enqueue_trial(
        {
            "bagging_fraction": 1.0,
            "bagging_freq": 0,
            "min_child_samples": 20,
        }
    )

    study.enqueue_trial(
        {
            "bagging_fraction": 0.75,
            "bagging_freq": 5,
            "min_child_samples": 20,
        }
    )

    import logging
    import sys

    # Add stream handler of stdout to show the messages to see Optuna works expectedly.
    optuna.logging.get_logger("optuna").addHandler(logging.StreamHandler(sys.stdout))
    study.optimize(objective, n_trials=100, timeout=600)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/runner/work/optuna-versioned-gh-pages/optuna-versioned-gh-pages/tutorial/20_recipes/008_specify_params.py:81: ExperimentalWarning:

    enqueue_trial is experimental (supported from v1.2.0). The interface can change in the future.

    /opt/hostedtoolcache/Python/3.7.10/x64/lib/python3.7/site-packages/optuna/study.py:858: ExperimentalWarning:

    create_trial is experimental (supported from v2.0.0). The interface can change in the future.

    /opt/hostedtoolcache/Python/3.7.10/x64/lib/python3.7/site-packages/optuna/study.py:858: ExperimentalWarning:

    add_trial is experimental (supported from v2.0.0). The interface can change in the future.

    /home/runner/work/optuna-versioned-gh-pages/optuna-versioned-gh-pages/tutorial/20_recipes/008_specify_params.py:89: ExperimentalWarning:

    enqueue_trial is experimental (supported from v1.2.0). The interface can change in the future.

    Trial 0 finished with value: 0.972027972027972 and parameters: {'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20}. Best is trial 0 with value: 0.972027972027972.
    Trial 1 finished with value: 0.986013986013986 and parameters: {'bagging_fraction': 0.75, 'bagging_freq': 5, 'min_child_samples': 20}. Best is trial 1 with value: 0.986013986013986.
    Trial 2 finished with value: 0.9790209790209791 and parameters: {'bagging_fraction': 0.8493207470974946, 'bagging_freq': 7, 'min_child_samples': 43}. Best is trial 1 with value: 0.986013986013986.
    Trial 3 finished with value: 0.972027972027972 and parameters: {'bagging_fraction': 0.49943530877774345, 'bagging_freq': 7, 'min_child_samples': 82}. Best is trial 1 with value: 0.986013986013986.
    Trial 4 finished with value: 0.951048951048951 and parameters: {'bagging_fraction': 0.7682878581934365, 'bagging_freq': 3, 'min_child_samples': 78}. Best is trial 1 with value: 0.986013986013986.
    Trial 5 pruned. Trial was pruned at iteration 31.
    Trial 6 pruned. Trial was pruned at iteration 0.
    Trial 7 pruned. Trial was pruned at iteration 0.
    Trial 8 pruned. Trial was pruned at iteration 0.
    Trial 9 pruned. Trial was pruned at iteration 0.
    Trial 10 pruned. Trial was pruned at iteration 0.
    Trial 11 finished with value: 0.965034965034965 and parameters: {'bagging_fraction': 0.8804988460496276, 'bagging_freq': 7, 'min_child_samples': 51}. Best is trial 1 with value: 0.986013986013986.
    Trial 12 pruned. Trial was pruned at iteration 4.
    Trial 13 finished with value: 0.986013986013986 and parameters: {'bagging_fraction': 0.8736867622606715, 'bagging_freq': 5, 'min_child_samples': 36}. Best is trial 1 with value: 0.986013986013986.
    Trial 14 pruned. Trial was pruned at iteration 0.
    Trial 15 pruned. Trial was pruned at iteration 63.
    Trial 16 pruned. Trial was pruned at iteration 30.
    Trial 17 pruned. Trial was pruned at iteration 4.
    Trial 18 pruned. Trial was pruned at iteration 0.
    Trial 19 pruned. Trial was pruned at iteration 0.
    Trial 20 pruned. Trial was pruned at iteration 0.
    Trial 21 pruned. Trial was pruned at iteration 0.
    Trial 22 pruned. Trial was pruned at iteration 0.
    Trial 23 pruned. Trial was pruned at iteration 4.
    Trial 24 pruned. Trial was pruned at iteration 0.
    Trial 25 finished with value: 0.9790209790209791 and parameters: {'bagging_fraction': 0.9855264239398265, 'bagging_freq': 7, 'min_child_samples': 26}. Best is trial 1 with value: 0.986013986013986.
    Trial 26 pruned. Trial was pruned at iteration 8.
    Trial 27 pruned. Trial was pruned at iteration 0.
    Trial 28 pruned. Trial was pruned at iteration 0.
    Trial 29 pruned. Trial was pruned at iteration 0.
    Trial 30 pruned. Trial was pruned at iteration 0.
    Trial 31 pruned. Trial was pruned at iteration 0.
    Trial 32 finished with value: 0.965034965034965 and parameters: {'bagging_fraction': 0.8940095237758585, 'bagging_freq': 7, 'min_child_samples': 14}. Best is trial 1 with value: 0.986013986013986.
    Trial 33 pruned. Trial was pruned at iteration 0.
    Trial 34 pruned. Trial was pruned at iteration 0.
    Trial 35 pruned. Trial was pruned at iteration 0.
    Trial 36 pruned. Trial was pruned at iteration 0.
    Trial 37 pruned. Trial was pruned at iteration 0.
    Trial 38 pruned. Trial was pruned at iteration 0.
    Trial 39 pruned. Trial was pruned at iteration 0.
    Trial 40 pruned. Trial was pruned at iteration 0.
    Trial 41 pruned. Trial was pruned at iteration 0.
    Trial 42 finished with value: 0.9790209790209791 and parameters: {'bagging_fraction': 0.9701158497458594, 'bagging_freq': 0, 'min_child_samples': 31}. Best is trial 1 with value: 0.986013986013986.
    Trial 43 pruned. Trial was pruned at iteration 0.
    Trial 44 pruned. Trial was pruned at iteration 0.
    Trial 45 pruned. Trial was pruned at iteration 0.
    Trial 46 pruned. Trial was pruned at iteration 0.
    Trial 47 finished with value: 0.9790209790209791 and parameters: {'bagging_fraction': 0.9390051541609252, 'bagging_freq': 6, 'min_child_samples': 23}. Best is trial 1 with value: 0.986013986013986.
    Trial 48 pruned. Trial was pruned at iteration 0.
    Trial 49 pruned. Trial was pruned at iteration 0.
    Trial 50 pruned. Trial was pruned at iteration 0.
    Trial 51 pruned. Trial was pruned at iteration 0.
    Trial 52 pruned. Trial was pruned at iteration 0.
    Trial 53 pruned. Trial was pruned at iteration 0.
    Trial 54 pruned. Trial was pruned at iteration 0.
    Trial 55 finished with value: 0.986013986013986 and parameters: {'bagging_fraction': 0.8912648255211676, 'bagging_freq': 6, 'min_child_samples': 45}. Best is trial 1 with value: 0.986013986013986.
    Trial 56 pruned. Trial was pruned at iteration 0.
    Trial 57 pruned. Trial was pruned at iteration 0.
    Trial 58 pruned. Trial was pruned at iteration 10.
    Trial 59 pruned. Trial was pruned at iteration 0.
    Trial 60 pruned. Trial was pruned at iteration 0.
    Trial 61 pruned. Trial was pruned at iteration 0.
    Trial 62 pruned. Trial was pruned at iteration 0.
    Trial 63 pruned. Trial was pruned at iteration 0.
    Trial 64 finished with value: 0.9790209790209791 and parameters: {'bagging_fraction': 0.9356125202467659, 'bagging_freq': 7, 'min_child_samples': 54}. Best is trial 1 with value: 0.986013986013986.
    Trial 65 pruned. Trial was pruned at iteration 0.
    Trial 66 pruned. Trial was pruned at iteration 0.
    Trial 67 pruned. Trial was pruned at iteration 0.
    Trial 68 pruned. Trial was pruned at iteration 0.
    Trial 69 pruned. Trial was pruned at iteration 0.
    Trial 70 pruned. Trial was pruned at iteration 0.
    Trial 71 finished with value: 0.9790209790209791 and parameters: {'bagging_fraction': 0.9292027276446094, 'bagging_freq': 7, 'min_child_samples': 22}. Best is trial 1 with value: 0.986013986013986.
    Trial 72 pruned. Trial was pruned at iteration 0.
    Trial 73 pruned. Trial was pruned at iteration 0.
    Trial 74 pruned. Trial was pruned at iteration 0.
    Trial 75 pruned. Trial was pruned at iteration 0.
    Trial 76 pruned. Trial was pruned at iteration 0.
    Trial 77 pruned. Trial was pruned at iteration 53.
    Trial 78 pruned. Trial was pruned at iteration 0.
    Trial 79 pruned. Trial was pruned at iteration 0.
    Trial 80 pruned. Trial was pruned at iteration 0.
    Trial 81 pruned. Trial was pruned at iteration 0.
    Trial 82 pruned. Trial was pruned at iteration 0.
    Trial 83 pruned. Trial was pruned at iteration 0.
    Trial 84 pruned. Trial was pruned at iteration 0.
    Trial 85 pruned. Trial was pruned at iteration 0.
    Trial 86 pruned. Trial was pruned at iteration 0.
    Trial 87 pruned. Trial was pruned at iteration 0.
    Trial 88 pruned. Trial was pruned at iteration 0.
    Trial 89 pruned. Trial was pruned at iteration 0.
    Trial 90 pruned. Trial was pruned at iteration 0.
    Trial 91 pruned. Trial was pruned at iteration 0.
    Trial 92 pruned. Trial was pruned at iteration 1.
    Trial 93 pruned. Trial was pruned at iteration 0.
    Trial 94 pruned. Trial was pruned at iteration 0.
    Trial 95 pruned. Trial was pruned at iteration 1.
    Trial 96 pruned. Trial was pruned at iteration 0.
    Trial 97 pruned. Trial was pruned at iteration 0.
    Trial 98 pruned. Trial was pruned at iteration 0.
    Trial 99 pruned. Trial was pruned at iteration 1.




.. GENERATED FROM PYTHON SOURCE LINES 101-112

Second scenario: Have Optuna utilize already evaluated hyperparameters
----------------------------------------------------------------------

In this scenario, let's assume you have some out-of-box sets of hyperparameters and
you have already evaluated them but the results are not desirable so that you are thinking of
using Optuna.

Optuna has :func:`optuna.study.Study.add_trial` which lets you register those results
to Optuna and then Optuna will sample hyperparameters taking them into account.

In this section,  the ``objective`` is the same as the first scenario.

.. GENERATED FROM PYTHON SOURCE LINES 112-145

.. code-block:: default


    study = optuna.create_study(direction="maximize", pruner=optuna.pruners.MedianPruner())
    study.add_trial(
        optuna.trial.create_trial(
            params={
                "bagging_fraction": 1.0,
                "bagging_freq": 0,
                "min_child_samples": 20,
            },
            distributions={
                "bagging_fraction": optuna.distributions.UniformDistribution(0.4, 1.0 + 1e-12),
                "bagging_freq": optuna.distributions.IntUniformDistribution(0, 7),
                "min_child_samples": optuna.distributions.IntUniformDistribution(5, 100),
            },
            value=0.94,
        )
    )
    study.add_trial(
        optuna.trial.create_trial(
            params={
                "bagging_fraction": 0.75,
                "bagging_freq": 5,
                "min_child_samples": 20,
            },
            distributions={
                "bagging_fraction": optuna.distributions.UniformDistribution(0.4, 1.0 + 1e-12),
                "bagging_freq": optuna.distributions.IntUniformDistribution(0, 7),
                "min_child_samples": optuna.distributions.IntUniformDistribution(5, 100),
            },
            value=0.95,
        )
    )
    study.optimize(objective, n_trials=100, timeout=600)




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    A new study created in memory with name: no-name-d9aabe6a-882e-4412-9bd1-fc5a455ff7f3
    /home/runner/work/optuna-versioned-gh-pages/optuna-versioned-gh-pages/tutorial/20_recipes/008_specify_params.py:126: ExperimentalWarning:

    create_trial is experimental (supported from v2.0.0). The interface can change in the future.

    /home/runner/work/optuna-versioned-gh-pages/optuna-versioned-gh-pages/tutorial/20_recipes/008_specify_params.py:126: ExperimentalWarning:

    add_trial is experimental (supported from v2.0.0). The interface can change in the future.

    /home/runner/work/optuna-versioned-gh-pages/optuna-versioned-gh-pages/tutorial/20_recipes/008_specify_params.py:141: ExperimentalWarning:

    create_trial is experimental (supported from v2.0.0). The interface can change in the future.

    /home/runner/work/optuna-versioned-gh-pages/optuna-versioned-gh-pages/tutorial/20_recipes/008_specify_params.py:141: ExperimentalWarning:

    add_trial is experimental (supported from v2.0.0). The interface can change in the future.

    Trial 2 finished with value: 0.951048951048951 and parameters: {'bagging_fraction': 0.43998327138856286, 'bagging_freq': 5, 'min_child_samples': 23}. Best is trial 2 with value: 0.951048951048951.
    Trial 3 finished with value: 0.972027972027972 and parameters: {'bagging_fraction': 0.5046965101574432, 'bagging_freq': 2, 'min_child_samples': 47}. Best is trial 3 with value: 0.972027972027972.
    Trial 4 finished with value: 0.972027972027972 and parameters: {'bagging_fraction': 0.798506338222814, 'bagging_freq': 3, 'min_child_samples': 73}. Best is trial 3 with value: 0.972027972027972.
    Trial 5 pruned. Trial was pruned at iteration 0.
    Trial 6 pruned. Trial was pruned at iteration 2.
    Trial 7 pruned. Trial was pruned at iteration 0.
    Trial 8 pruned. Trial was pruned at iteration 2.
    Trial 9 pruned. Trial was pruned at iteration 0.
    Trial 10 pruned. Trial was pruned at iteration 0.
    Trial 11 pruned. Trial was pruned at iteration 0.
    Trial 12 pruned. Trial was pruned at iteration 0.
    Trial 13 pruned. Trial was pruned at iteration 2.
    Trial 14 pruned. Trial was pruned at iteration 0.
    Trial 15 pruned. Trial was pruned at iteration 2.
    Trial 16 pruned. Trial was pruned at iteration 0.
    Trial 17 pruned. Trial was pruned at iteration 0.
    Trial 18 pruned. Trial was pruned at iteration 18.
    Trial 19 pruned. Trial was pruned at iteration 0.
    Trial 20 pruned. Trial was pruned at iteration 2.
    Trial 21 pruned. Trial was pruned at iteration 2.
    Trial 22 pruned. Trial was pruned at iteration 0.
    Trial 23 pruned. Trial was pruned at iteration 1.
    Trial 24 pruned. Trial was pruned at iteration 0.
    Trial 25 pruned. Trial was pruned at iteration 0.
    Trial 26 pruned. Trial was pruned at iteration 2.
    Trial 27 pruned. Trial was pruned at iteration 0.
    Trial 28 pruned. Trial was pruned at iteration 0.
    Trial 29 pruned. Trial was pruned at iteration 1.
    Trial 30 pruned. Trial was pruned at iteration 0.
    Trial 31 pruned. Trial was pruned at iteration 2.
    Trial 32 finished with value: 0.9790209790209791 and parameters: {'bagging_fraction': 0.8679436226640693, 'bagging_freq': 6, 'min_child_samples': 17}. Best is trial 32 with value: 0.9790209790209791.
    Trial 33 pruned. Trial was pruned at iteration 1.
    Trial 34 pruned. Trial was pruned at iteration 2.
    Trial 35 pruned. Trial was pruned at iteration 0.
    Trial 36 pruned. Trial was pruned at iteration 0.
    Trial 37 pruned. Trial was pruned at iteration 2.
    Trial 38 pruned. Trial was pruned at iteration 0.
    Trial 39 pruned. Trial was pruned at iteration 0.
    Trial 40 pruned. Trial was pruned at iteration 0.
    Trial 41 pruned. Trial was pruned at iteration 21.
    Trial 42 pruned. Trial was pruned at iteration 2.
    Trial 43 pruned. Trial was pruned at iteration 1.
    Trial 44 pruned. Trial was pruned at iteration 0.
    Trial 45 pruned. Trial was pruned at iteration 2.
    Trial 46 pruned. Trial was pruned at iteration 2.
    Trial 47 pruned. Trial was pruned at iteration 1.
    Trial 48 pruned. Trial was pruned at iteration 2.
    Trial 49 pruned. Trial was pruned at iteration 0.
    Trial 50 pruned. Trial was pruned at iteration 0.
    Trial 51 pruned. Trial was pruned at iteration 0.
    Trial 52 pruned. Trial was pruned at iteration 2.
    Trial 53 finished with value: 0.986013986013986 and parameters: {'bagging_fraction': 0.9627430655942251, 'bagging_freq': 1, 'min_child_samples': 28}. Best is trial 53 with value: 0.986013986013986.
    Trial 54 pruned. Trial was pruned at iteration 1.
    Trial 55 pruned. Trial was pruned at iteration 0.
    Trial 56 pruned. Trial was pruned at iteration 0.
    Trial 57 pruned. Trial was pruned at iteration 0.
    Trial 58 pruned. Trial was pruned at iteration 2.
    Trial 59 pruned. Trial was pruned at iteration 0.
    Trial 60 pruned. Trial was pruned at iteration 0.
    Trial 61 pruned. Trial was pruned at iteration 0.
    Trial 62 pruned. Trial was pruned at iteration 3.
    Trial 63 pruned. Trial was pruned at iteration 0.
    Trial 64 pruned. Trial was pruned at iteration 1.
    Trial 65 pruned. Trial was pruned at iteration 0.
    Trial 66 pruned. Trial was pruned at iteration 2.
    Trial 67 pruned. Trial was pruned at iteration 0.
    Trial 68 pruned. Trial was pruned at iteration 0.
    Trial 69 pruned. Trial was pruned at iteration 0.
    Trial 70 pruned. Trial was pruned at iteration 0.
    Trial 71 finished with value: 0.972027972027972 and parameters: {'bagging_fraction': 0.7026103188071341, 'bagging_freq': 6, 'min_child_samples': 20}. Best is trial 53 with value: 0.986013986013986.
    Trial 72 pruned. Trial was pruned at iteration 0.
    Trial 73 pruned. Trial was pruned at iteration 0.
    Trial 74 finished with value: 0.993006993006993 and parameters: {'bagging_fraction': 0.7080962347720234, 'bagging_freq': 7, 'min_child_samples': 20}. Best is trial 74 with value: 0.993006993006993.
    Trial 75 pruned. Trial was pruned at iteration 0.
    Trial 76 finished with value: 0.986013986013986 and parameters: {'bagging_fraction': 0.7401414538022023, 'bagging_freq': 7, 'min_child_samples': 15}. Best is trial 74 with value: 0.993006993006993.
    Trial 77 pruned. Trial was pruned at iteration 0.
    Trial 78 pruned. Trial was pruned at iteration 0.
    Trial 79 pruned. Trial was pruned at iteration 0.
    Trial 80 pruned. Trial was pruned at iteration 0.
    Trial 81 pruned. Trial was pruned at iteration 0.
    Trial 82 pruned. Trial was pruned at iteration 0.
    Trial 83 pruned. Trial was pruned at iteration 0.
    Trial 84 pruned. Trial was pruned at iteration 0.
    Trial 85 pruned. Trial was pruned at iteration 0.
    Trial 86 pruned. Trial was pruned at iteration 0.
    Trial 87 pruned. Trial was pruned at iteration 0.
    Trial 88 pruned. Trial was pruned at iteration 0.
    Trial 89 pruned. Trial was pruned at iteration 0.
    Trial 90 pruned. Trial was pruned at iteration 0.
    Trial 91 pruned. Trial was pruned at iteration 0.
    Trial 92 pruned. Trial was pruned at iteration 0.
    Trial 93 pruned. Trial was pruned at iteration 2.
    Trial 94 pruned. Trial was pruned at iteration 0.
    Trial 95 pruned. Trial was pruned at iteration 0.
    Trial 96 pruned. Trial was pruned at iteration 0.
    Trial 97 pruned. Trial was pruned at iteration 0.
    Trial 98 pruned. Trial was pruned at iteration 0.
    Trial 99 pruned. Trial was pruned at iteration 0.
    Trial 100 pruned. Trial was pruned at iteration 0.
    Trial 101 pruned. Trial was pruned at iteration 0.





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  7.443 seconds)


.. _sphx_glr_download_tutorial_20_recipes_008_specify_params.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: 008_specify_params.py <008_specify_params.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: 008_specify_params.ipynb <008_specify_params.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
